{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ujSQvNwNje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbcd9815-60bb-4322-bece-86adcd29160a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_cors pyngrok"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jwAISMZdy6Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c2K07zKlSDMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'\n",
        "\n",
        "# Puis accéder à ton token comme ça\n",
        "token = os.getenv('HF_TOKEN')\n",
        "print(token)\n"
      ],
      "metadata": {
        "id": "y_EpwK-aSgCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "q3w0vKDHii_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "_dvlQmPsimjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "\n",
        "hf_token = os.getenv('HF_TOKEN')  # ou remplace directement par une string : \"hf_xxxxxxxxxxxxx\"\n",
        "\n",
        "base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
        "\n",
        "# Config quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Charger le tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=hf_token)\n",
        "\n",
        "# Charger le modèle avec offload si besoin\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # dispatch automatique sur GPU/CPU\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "Gcr8JvpqxyvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers accelerate bitsandbytes flask flask_cors pyngrok -q\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 3. Auth Hugging Face\n",
        "from google.colab import userdata\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "# 4. Define prompt template\n",
        "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#  Function to format prompt and extract response\n",
        "def format_prompt_and_generate(question, max_new_tokens=150, temperature=0.7, top_p=0.9):\n",
        "    formatted_prompt = prompt_style.format(question, \"\")\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    generated_tokens = outputs.sequences[0][input_length:]\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "# 7. Création de l'app Flask\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def home():\n",
        "    return \"Medical AI Flask app is running!\"\n",
        "\n",
        "@app.route(\"/echo\", methods=[\"GET\"])\n",
        "def echo():\n",
        "    return \"echo\"\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"GET\", \"POST\"])\n",
        "def generate():\n",
        "    try:\n",
        "        # Get the medical question from request\n",
        "        if request.method == \"POST\":\n",
        "            data = request.get_json()\n",
        "            question = data.get(\"message\") or data.get(\"question\")\n",
        "        else:\n",
        "            question = request.args.get(\"message\") or request.args.get(\"question\")\n",
        "\n",
        "        if not question:\n",
        "            return jsonify({\"error\": \"No question provided. Use 'message' or 'question' parameter.\"}), 400\n",
        "\n",
        "        # Get optional parameters\n",
        "        max_tokens = int(request.json.get(\"max_tokens\", 150)) if request.method == \"POST\" else int(request.args.get(\"max_tokens\", 150))\n",
        "        temperature = float(request.json.get(\"temperature\", 0.7)) if request.method == \"POST\" else float(request.args.get(\"temperature\", 0.7))\n",
        "        top_p = float(request.json.get(\"top_p\", 0.9)) if request.method == \"POST\" else float(request.args.get(\"top_p\", 0.9))\n",
        "\n",
        "        # Generate response using the formatted prompt\n",
        "        response = format_prompt_and_generate(\n",
        "            question=question,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p\n",
        "        )\n",
        "\n",
        "        return jsonify({\n",
        "            \"question\": question,\n",
        "            \"response\": response,\n",
        "            \"parameters\": {\n",
        "                \"max_tokens\": max_tokens,\n",
        "                \"temperature\": temperature,\n",
        "                \"top_p\": top_p\n",
        "            }\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health() :\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"model\": base_model_name,\n",
        "        \"device\": str(model.device) if hasattr(model, 'device') else \"unknown\"\n",
        "    })"
      ],
      "metadata": {
        "id": "kab3SsYUy1uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['ngrok_token'] = '2xbbSsskcpCrAFIsYy1tcrivdLo_7F6WPKMF3NcBwVepMZ6xQ'\n",
        "\n",
        "# Puis accéder à ton token comme ça\n",
        "token = os.getenv('ngrok_token')\n",
        "print(token)"
      ],
      "metadata": {
        "id": "wBxh4Dwtbpq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lancer ngrokc pour exposer l’API\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Stop any running ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok_token = os.getenv('ngrok_token')\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "port = 500\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"URL publique : {public_url}\")\n",
        "\n",
        "\n",
        "def run():\n",
        "    app.run(port=port)\n",
        "\n",
        "threading.Thread(target=run).start()"
      ],
      "metadata": {
        "id": "LHuWYrS1zLsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgmeyL20MDuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}